{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack, hstack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['backers_count', 'blurb', 'category', 'converted_pledged_amount',\n",
       "       'country', 'created_at', 'current_currency', 'deadline', 'fx_rate',\n",
       "       'goal', 'id', 'launched_at', 'location', 'name', 'pledged', 'profile',\n",
       "       'slug', 'source_url', 'spotlight', 'staff_pick', 'state',\n",
       "       'state_changed_at', 'static_usd_rate', 'urls', 'usd_pledged',\n",
       "       'usd_type', 'story', 'faq', 'num_faq', 'comments', 'n_comments',\n",
       "       'duration'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "dataset = pd.read_csv('Output/Combined_dataset.csv')\n",
    "dataset = dataset.drop('Unnamed: 0', axis=1)\n",
    "dataset['duration'] = pd.to_numeric(pd.to_timedelta(dataset['duration']).dt.days, downcast='integer')\n",
    "dataset['num_faq'] = dataset['num_faq'].astype(int)\n",
    "dataset['n_comments'] = dataset['n_comments'].str.replace(',', '').astype(float)\n",
    "\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# ['blurb', 'name', 'story', 'faq', 'comments']\n",
    "dataset['blurb_name'] = dataset['blurb'] + dataset['name']\n",
    "dataset['blurb_story'] = dataset['blurb'] + dataset['story']\n",
    "dataset['name_faq'] = dataset['name'] + dataset['faq']\n",
    "dataset['name_comments'] = dataset['name'] + dataset['comments']\n",
    "dataset['name_faq_comments'] = dataset['faq'] + dataset['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na\n",
    "dataset.story = dataset.story.fillna('na')\n",
    "\n",
    "\n",
    "# Decide whether to drop these columns later\n",
    "# cols_to_drop = ['profile', 'category', 'created_at', 'location', 'current_currency', 'deadline', 'id', 'launched_at', 'slug', 'source_url', 'state_changed_at', 'urls', 'static_usd_rate', 'usd_pledged', 'converted_pledged_amount', 'spotlight']\n",
    "\n",
    "# dataset = dataset.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = dataset['blurb']\n",
    "# y = dataset.state\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# X_train_dtm = vect.fit_transform(X_train)\n",
    "# X_test_dtm = vect.transform(X_test)\n",
    "# print(X_train_dtm.shape)\n",
    "# # print the number of features that were generated\n",
    "# print('Features: ', X_train_dtm.shape[1])\n",
    "\n",
    "# # use Multinomial Naive Bayes to predict the star rating\n",
    "# logreg = LogisticRegression()\n",
    "# logreg.fit(X_train_dtm, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "vect = CountVectorizer()\n",
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    \n",
    "    # create document-term matrices using the vectorizer\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    print(X_train_dtm.shape)\n",
    "    # print the number of features that were generated\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    \n",
    "    # use Multinomial Naive Bayes to predict the star rating\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = logreg.predict(X_test_dtm)\n",
    "    print(logreg.coef_)\n",
    "    # print(y_test)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "    print('Confusion Matrix: ', metrics.confusion_matrix(y_test, y_pred_class))\n",
    "    # print('Recall: ', metrics.recall_score(y_test, y_pred_class))\n",
    "    # print('Precision: ', metrics.precision_score(y_test, y_pred_class))\n",
    "    # print('AUC Score: ', metrics.roc_auc_score(y_test, y_pred_class))\n",
    "    # print('F1 Score: ', metrics.f1_score(y_test, y_pred_class))\n",
    "    print('Classification Report: ', metrics.classification_report(y_test, y_pred_class))\n",
    "    # metrics.plot_precision_recall_curve(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature is blurb\n",
      "(750, 4228)\n",
      "Features:  4228\n",
      "[[-0.30705723 -0.26735382 -0.42299032 ... -0.1781696   0.10571352\n",
      "   0.14846353]]\n",
      "Accuracy:  0.648\n",
      "Confusion Matrix:  [[ 33  66]\n",
      " [ 22 129]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "      failed       0.60      0.33      0.43        99\n",
      "  successful       0.66      0.85      0.75       151\n",
      "\n",
      "    accuracy                           0.65       250\n",
      "   macro avg       0.63      0.59      0.59       250\n",
      "weighted avg       0.64      0.65      0.62       250\n",
      "\n",
      "\n",
      "\n",
      "Feature is name\n",
      "(750, 2164)\n",
      "Features:  2164\n",
      "[[ 0.14126062  0.08513279 -0.24242734 ...  0.18366959 -0.27954043\n",
      "  -0.26981244]]\n",
      "Accuracy:  0.584\n",
      "Confusion Matrix:  [[ 12  87]\n",
      " [ 17 134]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "      failed       0.41      0.12      0.19        99\n",
      "  successful       0.61      0.89      0.72       151\n",
      "\n",
      "    accuracy                           0.58       250\n",
      "   macro avg       0.51      0.50      0.45       250\n",
      "weighted avg       0.53      0.58      0.51       250\n",
      "\n",
      "\n",
      "\n",
      "Feature is story\n",
      "(750, 32088)\n",
      "Features:  32088\n",
      "[[-0.16767274 -0.120061   -0.00134673 ... -0.00274966 -0.00274966\n",
      "  -0.00274966]]\n",
      "Accuracy:  0.696\n",
      "Confusion Matrix:  [[ 49  50]\n",
      " [ 26 125]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "      failed       0.65      0.49      0.56        99\n",
      "  successful       0.71      0.83      0.77       151\n",
      "\n",
      "    accuracy                           0.70       250\n",
      "   macro avg       0.68      0.66      0.67       250\n",
      "weighted avg       0.69      0.70      0.69       250\n",
      "\n",
      "\n",
      "\n",
      "Feature is faq\n",
      "(750, 4522)\n",
      "Features:  4522\n",
      "[[ 0.01182388 -0.0621563   0.02331675 ...  0.00980491  0.00358029\n",
      "   0.00980491]]\n",
      "Accuracy:  0.604\n",
      "Confusion Matrix:  [[  0  99]\n",
      " [  0 151]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "      failed       0.00      0.00      0.00        99\n",
      "  successful       0.60      1.00      0.75       151\n",
      "\n",
      "    accuracy                           0.60       250\n",
      "   macro avg       0.30      0.50      0.38       250\n",
      "weighted avg       0.36      0.60      0.45       250\n",
      "\n",
      "\n",
      "\n",
      "Feature is comments\n",
      "(750, 12344)\n",
      "Features:  12344\n",
      "[[-9.97278454e-02 -2.05323469e-02  4.79888234e-07 ... -1.25511774e-01\n",
      "  -1.25511774e-01 -3.23071289e-05]]\n",
      "Accuracy:  0.748\n",
      "Confusion Matrix:  [[89 10]\n",
      " [53 98]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "      failed       0.63      0.90      0.74        99\n",
      "  successful       0.91      0.65      0.76       151\n",
      "\n",
      "    accuracy                           0.75       250\n",
      "   macro avg       0.77      0.77      0.75       250\n",
      "weighted avg       0.80      0.75      0.75       250\n",
      "\n",
      "\n",
      "\n",
      "Feature is blurb_name\n",
      "(750, 5064)\n",
      "Features:  5064\n",
      "[[ 0.03135182 -0.24772144 -0.16786372 ...  0.09177952  0.13453561\n",
      "  -0.25065845]]\n",
      "Accuracy:  0.628\n",
      "Confusion Matrix:  [[ 34  65]\n",
      " [ 28 123]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "      failed       0.55      0.34      0.42        99\n",
      "  successful       0.65      0.81      0.73       151\n",
      "\n",
      "    accuracy                           0.63       250\n",
      "   macro avg       0.60      0.58      0.57       250\n",
      "weighted avg       0.61      0.63      0.61       250\n",
      "\n",
      "\n",
      "\n",
      "Feature is blurb_story\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-06d577998499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# split X and y into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtokenize_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-6ea62cf536de>\u001b[0m in \u001b[0;36mtokenize_test\u001b[0;34m(vect)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# create document-term matrices using the vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_train_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX_test_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bernard.chua/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1204\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bernard.chua/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bernard.chua/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bernard.chua/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    218\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# X = dataset.drop('state', axis=1)\n",
    "text_cols = ['blurb', 'name', 'story', 'faq', 'comments', 'blurb_name', 'blurb_story','name_faq', 'name_comments', 'name_faq_comments']\n",
    "\n",
    "for feature in text_cols:\n",
    "    print(f\"Feature is {feature}\")\n",
    "    X = dataset[feature]\n",
    "    y = dataset.state\n",
    "\n",
    "    # split X and y into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    tokenize_test(vect)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "afinn_scores = [afinn.score(text) for text in dataset.story]\n",
    "dataset['afinn'] = afinn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   backers_count                                              blurb  \\\n",
       "0              1  With your help we will create this device that...   \n",
       "1              2  We at Ormiston Primary are looking at starting...   \n",
       "2              0  Self-taught aspiring metalsmith Looking for he...   \n",
       "3              0  So many women believe they are past their prim...   \n",
       "4             10  The Horror Zine's Jeani Rector brings us anoth...   \n",
       "\n",
       "                                            category  \\\n",
       "0  {\"id\":331,\"name\":\"3D Printing\",\"slug\":\"technol...   \n",
       "1  {\"id\":309,\"name\":\"Farms\",\"slug\":\"food/farms\",\"...   \n",
       "2  {\"id\":54,\"name\":\"Mixed Media\",\"slug\":\"art/mixe...   \n",
       "3  {\"id\":278,\"name\":\"People\",\"slug\":\"photography/...   \n",
       "4  {\"id\":324,\"name\":\"Anthologies\",\"slug\":\"publish...   \n",
       "\n",
       "   converted_pledged_amount country        created_at current_currency  \\\n",
       "0                         1      ES  2015-08-18 21:01              USD   \n",
       "1                         9      NZ  2015-08-11 18:04              USD   \n",
       "2                         0      US  2015-04-28 21:14              USD   \n",
       "3                         0      US   2014-07-07 1:30              USD   \n",
       "4                       340      US  2014-11-04 16:30              USD   \n",
       "\n",
       "           deadline   fx_rate   goal  ...  num_faq  \\\n",
       "0  2016-07-09 20:11  1.212886  15000  ...        0   \n",
       "1  2015-09-11 15:55  0.723585   5000  ...        0   \n",
       "2  2015-05-28 21:14  1.000000  10000  ...        0   \n",
       "3   2014-10-26 0:00  1.000000   2000  ...        0   \n",
       "4   2014-12-09 9:20  1.000000   2500  ...        0   \n",
       "\n",
       "                                            comments n_comments duration  \\\n",
       "0  Only backers can post comments. Log in\\nNo com...        0.0       30   \n",
       "1  Only backers can post comments. Log in\\nNo com...        0.0       30   \n",
       "2  Only backers can post comments. Log in\\nNo com...        0.0       29   \n",
       "3  Only backers can post comments. Log in\\nNo com...        0.0       30   \n",
       "4  Only backers can post comments. Log in\\nPaula ...        1.0       30   \n",
       "\n",
       "                                          blurb_name  \\\n",
       "0  With your help we will create this device that...   \n",
       "1  We at Ormiston Primary are looking at starting...   \n",
       "2  Self-taught aspiring metalsmith Looking for he...   \n",
       "3  So many women believe they are past their prim...   \n",
       "4  The Horror Zine's Jeani Rector brings us anoth...   \n",
       "\n",
       "                                         blurb_story  \\\n",
       "0  With your help we will create this device that...   \n",
       "1  We at Ormiston Primary are looking at starting...   \n",
       "2  Self-taught aspiring metalsmith Looking for he...   \n",
       "3  So many women believe they are past their prim...   \n",
       "4  The Horror Zine's Jeani Rector brings us anoth...   \n",
       "\n",
       "                                            name_faq  \\\n",
       "0  Save water 100% liquid downloads in toilets at...   \n",
       "1           Ormiston Primary Community Garden['   ']   \n",
       "2  Aspiring metalsmith in need of better tools, a...   \n",
       "3                           Beauty At Any Age['   ']   \n",
       "4    Shrieks and Shivers from the Horror Zine['   ']   \n",
       "\n",
       "                                       name_comments  \\\n",
       "0  Save water 100% liquid downloads in toilets at...   \n",
       "1  Ormiston Primary Community GardenOnly backers ...   \n",
       "2  Aspiring metalsmith in need of better tools, a...   \n",
       "3  Beauty At Any AgeOnly backers can post comment...   \n",
       "4  Shrieks and Shivers from the Horror ZineOnly b...   \n",
       "\n",
       "                                   name_faq_comments  afinn  \n",
       "0  ['   ']Only backers can post comments. Log in\\...  -10.0  \n",
       "1  ['   ']Only backers can post comments. Log in\\...   15.0  \n",
       "2  ['   ']Only backers can post comments. Log in\\...   19.0  \n",
       "3  ['   ']Only backers can post comments. Log in\\...   14.0  \n",
       "4  ['   ']Only backers can post comments. Log in\\...    2.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>backers_count</th>\n      <th>blurb</th>\n      <th>category</th>\n      <th>converted_pledged_amount</th>\n      <th>country</th>\n      <th>created_at</th>\n      <th>current_currency</th>\n      <th>deadline</th>\n      <th>fx_rate</th>\n      <th>goal</th>\n      <th>...</th>\n      <th>num_faq</th>\n      <th>comments</th>\n      <th>n_comments</th>\n      <th>duration</th>\n      <th>blurb_name</th>\n      <th>blurb_story</th>\n      <th>name_faq</th>\n      <th>name_comments</th>\n      <th>name_faq_comments</th>\n      <th>afinn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>With your help we will create this device that...</td>\n      <td>{\"id\":331,\"name\":\"3D Printing\",\"slug\":\"technol...</td>\n      <td>1</td>\n      <td>ES</td>\n      <td>2015-08-18 21:01</td>\n      <td>USD</td>\n      <td>2016-07-09 20:11</td>\n      <td>1.212886</td>\n      <td>15000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>Only backers can post comments. Log in\\nNo com...</td>\n      <td>0.0</td>\n      <td>30</td>\n      <td>With your help we will create this device that...</td>\n      <td>With your help we will create this device that...</td>\n      <td>Save water 100% liquid downloads in toilets at...</td>\n      <td>Save water 100% liquid downloads in toilets at...</td>\n      <td>['   ']Only backers can post comments. Log in\\...</td>\n      <td>-10.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>We at Ormiston Primary are looking at starting...</td>\n      <td>{\"id\":309,\"name\":\"Farms\",\"slug\":\"food/farms\",\"...</td>\n      <td>9</td>\n      <td>NZ</td>\n      <td>2015-08-11 18:04</td>\n      <td>USD</td>\n      <td>2015-09-11 15:55</td>\n      <td>0.723585</td>\n      <td>5000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>Only backers can post comments. Log in\\nNo com...</td>\n      <td>0.0</td>\n      <td>30</td>\n      <td>We at Ormiston Primary are looking at starting...</td>\n      <td>We at Ormiston Primary are looking at starting...</td>\n      <td>Ormiston Primary Community Garden['   ']</td>\n      <td>Ormiston Primary Community GardenOnly backers ...</td>\n      <td>['   ']Only backers can post comments. Log in\\...</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Self-taught aspiring metalsmith Looking for he...</td>\n      <td>{\"id\":54,\"name\":\"Mixed Media\",\"slug\":\"art/mixe...</td>\n      <td>0</td>\n      <td>US</td>\n      <td>2015-04-28 21:14</td>\n      <td>USD</td>\n      <td>2015-05-28 21:14</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>Only backers can post comments. Log in\\nNo com...</td>\n      <td>0.0</td>\n      <td>29</td>\n      <td>Self-taught aspiring metalsmith Looking for he...</td>\n      <td>Self-taught aspiring metalsmith Looking for he...</td>\n      <td>Aspiring metalsmith in need of better tools, a...</td>\n      <td>Aspiring metalsmith in need of better tools, a...</td>\n      <td>['   ']Only backers can post comments. Log in\\...</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>So many women believe they are past their prim...</td>\n      <td>{\"id\":278,\"name\":\"People\",\"slug\":\"photography/...</td>\n      <td>0</td>\n      <td>US</td>\n      <td>2014-07-07 1:30</td>\n      <td>USD</td>\n      <td>2014-10-26 0:00</td>\n      <td>1.000000</td>\n      <td>2000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>Only backers can post comments. Log in\\nNo com...</td>\n      <td>0.0</td>\n      <td>30</td>\n      <td>So many women believe they are past their prim...</td>\n      <td>So many women believe they are past their prim...</td>\n      <td>Beauty At Any Age['   ']</td>\n      <td>Beauty At Any AgeOnly backers can post comment...</td>\n      <td>['   ']Only backers can post comments. Log in\\...</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>The Horror Zine's Jeani Rector brings us anoth...</td>\n      <td>{\"id\":324,\"name\":\"Anthologies\",\"slug\":\"publish...</td>\n      <td>340</td>\n      <td>US</td>\n      <td>2014-11-04 16:30</td>\n      <td>USD</td>\n      <td>2014-12-09 9:20</td>\n      <td>1.000000</td>\n      <td>2500</td>\n      <td>...</td>\n      <td>0</td>\n      <td>Only backers can post comments. Log in\\nPaula ...</td>\n      <td>1.0</td>\n      <td>30</td>\n      <td>The Horror Zine's Jeani Rector brings us anoth...</td>\n      <td>The Horror Zine's Jeani Rector brings us anoth...</td>\n      <td>Shrieks and Shivers from the Horror Zine['   ']</td>\n      <td>Shrieks and Shivers from the Horror ZineOnly b...</td>\n      <td>['   ']Only backers can post comments. Log in\\...</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 38 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# nlp(dataset.story[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do regularization"
   ]
  }
 ]
}